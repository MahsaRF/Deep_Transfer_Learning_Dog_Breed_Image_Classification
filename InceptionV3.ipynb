{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InceptionV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNRkpFdzxLEmKPfN/YB29BZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "STZx3PUZEJdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.models import Model\n",
        "from tensorflow.python.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
        "from tensorflow.python.keras.optimizers import Adam\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import Sequence\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from pylab import rcParams"
      ],
      "metadata": {
        "id": "K6IgIIdlEYfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Exploration"
      ],
      "metadata": {
        "id": "dhfLp5yKEWZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "breed_list = os.listdir(\"images/\")\n",
        "number_of_used_breed =40\n",
        "from PIL import Image\n",
        "breed_in_use = 1"
      ],
      "metadata": {
        "id": "go9vtiPREho_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating training and testing dataset"
      ],
      "metadata": {
        "id": "qvZryoFvEkiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('Mydata',exist_ok= True)\n",
        "os.makedirs('Mydata/Train',exist_ok= True)\n",
        "os.makedirs('Mydata/Valid',exist_ok= True)\n",
        "for breed in breed_list:\n",
        "    os.makedirs('Mydata/Train/' + breed,exist_ok= True)\n",
        "    os.makedirs('Mydata/Valid/' + breed,exist_ok= True)\n",
        "    if breed_in_use == number_of_used_breed:\n",
        "        break\n",
        "    breed_in_use = breed_in_use+1\n",
        "print('Created {} folders to store Training images of the different breeds.'.format(len(os.listdir('Mydata/Train'))))\n",
        "print('Created {} folders to store Validation images of the different breeds.'.format(len(os.listdir('Mydata/Valid'))))\n",
        "\n",
        "validation_to_training_ratio = .1\n",
        "breed_in_use = 1\n",
        "for breed in os.listdir('Mydata/Train'):\n",
        "    cpt = sum([len(files) for r, d, files in os.walk('images/{}/'.format(breed))])\n",
        "    validation = (int)(cpt * validation_to_training_ratio)\n",
        "    index = 0\n",
        "    for file in os.listdir('images/{}'.format(breed)):\n",
        "        img = Image.open('images/{}/{}'.format(breed, file))\n",
        "        img = img.convert('RGB')        \n",
        "        if index<validation:\n",
        "            img.save('Mydata/Valid/' + breed + '/' + file + '.jpg')\n",
        "        else:\n",
        "            img.save('Mydata/Train/' + breed + '/' + file + '.jpg')\n",
        "        index = index +1\n",
        "    if breed_in_use == number_of_used_breed:\n",
        "        break    \n",
        "    breed_in_use = breed_in_use+1\n",
        "\n",
        "train_dir = 'Mydata/Train'\n",
        "validation_dir = 'Mydata/Valid'\n",
        "train_batchsize = 50\n",
        "val_batchsize = 10"
      ],
      "metadata": {
        "id": "vsOZFYgqEqSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Custom Dataset"
      ],
      "metadata": {
        "id": "TtBd9MlbFZHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add our data-augmentation parameters to ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   rotation_range = 40,\n",
        "                                   width_shift_range = 0.2,\n",
        "                                   height_shift_range = 0.2,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "# Note that the validation data should not be augmented!\n",
        "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# Flow training images in batches of 50 using train_datagen generator\n",
        "train_batches  = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size = train_batchsize,\n",
        "                                                    class_mode = 'categorical',\n",
        "                                                    target_size = (150, 150))     \n",
        "\n",
        "# Flow validation images in batches of 10 using test_datagen generator\n",
        "valid_batches =  test_datagen.flow_from_directory( validation_dir,\n",
        "                                                          batch_size  = val_batchsize,\n",
        "                                                          class_mode  = 'categorical',\n",
        "                                                          target_size = (150, 150),\n",
        "                                                          shuffle=False)\n",
        "\n",
        "train_sample = sum([len(files) for r, d, files in os.walk(train_dir)])\n",
        "validation_sample = sum([len(files) for r, d, files in os.walk(validation_dir)])\n",
        "\n",
        "print('****************')\n",
        "for cls, idx in train_batches.class_indices.items():\n",
        "    print('Class #{} = {}'.format(idx, cls))\n",
        "print('****************')"
      ],
      "metadata": {
        "id": "8uLf5VJqFf03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Architecture\n",
        "\n",
        "# Inception V3 pre-trained model\n"
      ],
      "metadata": {
        "id": "AnFQV6jWFsUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE    = (150, 150)\n",
        "InceptionV3_pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "                                include_top = False)\n",
        "\n",
        "for layer in InceptionV3_pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "InceptionV3_pre_trained_model.summary()\n",
        "\n",
        "#last_layer_InceptionResV2 = net.get_layer('block8_9')\n",
        "last_layer_InceptionV3 = InceptionV3_pre_trained_model.get_layer('mixed7')\n",
        "\n",
        "print('InceptionV3 last layer output shape: ', last_layer_InceptionV3.output_shape)\n",
        "InceptionV3_last_output = last_layer_InceptionV3.output\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x_InceptionV3 = layers.Flatten()(InceptionV3_last_output)\n",
        "# Add a fully connected layer with 512 hidden units and sigmoid activation\n",
        "x_InceptionV3 = layers.Dense(512, activation='sigmoid')(x_InceptionV3)\n",
        "# Add a dropout rate of 0.2\n",
        "x_InceptionV3 = layers.Dropout(0.2)(x_InceptionV3)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x_InceptionV3 = layers.Dense  (number_of_used_breed, activation='softmax')(x_InceptionV3)  \n",
        "\n",
        "InceptionV3_model = Model( InceptionV3_pre_trained_model.input, x_InceptionV3) \n",
        "InceptionV3_model.compile(optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), \n",
        "              loss = 'categorical_crossentropy', \n",
        "              metrics = ['acc'])\n",
        "InceptionV3_model.summary()\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('acc')>0.95):\n",
        "            print('/nTraining will stop as we have reached 95% of acc')\n",
        "            self.model.stop_training=True\n",
        "        \n",
        "callback=myCallback()"
      ],
      "metadata": {
        "id": "2fmfKO_AFv7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "pDNk8pAOHCxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_InceptionV3 = InceptionV3_model.fit_generator(\n",
        "            train_batches,\n",
        "            validation_data = valid_batches,\n",
        "            steps_per_epoch = train_sample // train_batchsize,\n",
        "            epochs =20,\n",
        "            validation_steps = validation_sample//val_batchsize,\n",
        "            verbose = 2,\n",
        "            callbacks=[callback]\n",
        ")\n",
        "\n",
        "acc = history_InceptionV3.history['acc']\n",
        "val_acc = history_InceptionV3.history['val_acc']\n",
        "loss = history_InceptionV3.history['loss']\n",
        "val_loss = history_InceptionV3.history['val_loss']"
      ],
      "metadata": {
        "id": "5EsbSgZkHE3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting history\n",
        "# Accuracy v/s Epochs plot"
      ],
      "metadata": {
        "id": "R3DxR4b4HVQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy_InceptionV3')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t8ZocjwXHhK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "cMyEewy2Hnxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_steps_per_epoch = numpy.math.ceil(validation_sample / val_batchsize)\n",
        "\n",
        "predictions = InceptionV3_model.predict_generator(valid_batches, steps=test_steps_per_epoch)\n",
        "# Get most likely class\n",
        "predicted_classes = numpy.argmax(predictions, axis=1)\n",
        "\n",
        "l=list(valid_batches.class_indices.keys())\n",
        "split_list =[i.split() for i in l]\n",
        "class_labels=[]\n",
        "for i in (range(len(split_list))):\n",
        "    class_labels.append(split_list[i][0].split(\"-\",1)[1])\n",
        "\n",
        "true_classes = valid_batches.classes\n",
        "report = classification_report(true_classes, predicted_classes, target_names=class_labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "1WAwbqHNIDIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "3VgM2wzBIJ6Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQb9g9fDU6k"
      },
      "outputs": [],
      "source": [
        "cm= confusion_matrix(true_classes,predicted_classes,labels=np.unique(true_classes))\n",
        "print(cm)\n",
        "\n",
        "rcParams['figure.figsize'] = 10, 10\n",
        "plt.style.use('ggplot')\n",
        "fig, ax = plt.subplots(figsize=(18, 18))\n",
        "_ = sns.heatmap(cm, ax=ax, yticklabels=class_labels, xticklabels=class_labels, robust=True)\n",
        "\n",
        "ax.set_title('Confusion matrix')\n",
        "#set ticklabels rotation\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90, fontsize = 10)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation = 0, fontsize = 10)\n",
        "\n",
        "\n"
      ]
    }
  ]
}